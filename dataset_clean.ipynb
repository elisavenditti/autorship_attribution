{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECEDENTE METODO PER PRENDERE LA FREQUENZA DEI TERMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "documents = [\n",
    "    \"It is wonderful day day.\",\n",
    "    \"My dear! Wonderful days will come.\",\n",
    "]\n",
    "\n",
    "# dividiamo il training set in tanti corpus, uno per ogni autore\n",
    "corpus = df['text'].to_numpy()\n",
    "items = len(corpus)\n",
    "authors = 50\n",
    "# authors = df['author'].nunique()\n",
    "# print(\"n autori: {}\".format(authors))\n",
    "\n",
    "\n",
    "corpuses = {}\n",
    "# inizializzazione della struttura dati\n",
    "for i in range (1, authors+1):\n",
    "    corpuses[i] = []\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    corpuses[row[\"author\"]].append(row[\"text\"])\n",
    "    \n",
    "# stampa di prova, il dataset è troppo grande e provo a stampare solo qualcosa\n",
    "print(\"5 elementi del corpus classe 1: {}\".format(corpuses[1][1:5]))\n",
    "\n",
    "\n",
    "# normalizzo l'id degli autori per eliminare quelli che non hanno testi\n",
    "keys_to_pop = []\n",
    "for key in corpuses.keys():\n",
    "    if len(corpuses[key])==0:\n",
    "        print(\"chiave {} no testi\\n\".format(key))\n",
    "        keys_to_pop.append(key)\n",
    "\n",
    "for key in keys_to_pop:\n",
    "    corpuses.pop(key)\n",
    "\n",
    "new_corpuses = {indice + 1: valore for indice, valore in enumerate(corpuses.values())}\n",
    "print(new_corpuses.keys())\n",
    "    \n",
    "\n",
    "\n",
    "# funzione per la lemmatizzazione di un testo in formato stringa\n",
    "def lemmatize_text(text, lemmatizer):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "# creazione della distribuzione delle parole per i vari autori\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "for i in range (1, 1):#authors+1):\n",
    "    author_corpus = corpuses[i]\n",
    "    lemmatized_data = [lemmatize_text(text, lemmatizer) for text in author_corpus]\n",
    "\n",
    "    vectorized_data = vectorizer.fit_transform(lemmatized_data)\n",
    "\n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "    # print(vectorized_data.toarray())\n",
    "    print(\"Language model for author {} : {}\".format(i,vectorized_data.toarray().sum(axis=0)))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USO DEI THREAD PER CALCOLARE UN MODELLO DI LINGUAGGIO PER OGNI AUTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = []\n",
    "for i in range (0,authors):\n",
    "    language_model.append([])\n",
    "\n",
    "def compute(i):\n",
    "    start = 1 + (i-1)*(authors//n)\n",
    "    end = 1 + i*(authors//n)\n",
    "    if i==n:\n",
    "        end += authors%n\n",
    "    for z in range(start,end):\n",
    "        author_corpus = corpuses[z]\n",
    "        lemmatized_data = [lemmatize_text(text, lemmatizer) for text in author_corpus]\n",
    "        vectorized_data = vectorizer.fit_transform(lemmatized_data)\n",
    "\n",
    "        temp = []\n",
    "        temp.append(vectorizer.get_feature_names_out())\n",
    "        temp.append(vectorized_data.toarray().sum(axis=0))\n",
    "        # print(vectorizer.get_feature_names_out())\n",
    "        # print(vectorized_data.toarray())\n",
    "        # print(\"Language model for author {} : {}\".format(z,vectorized_data.toarray().sum(axis=0)))\n",
    "        language_model[z-1].append(temp)\n",
    "        print(\"{}/{}\".format(z,end-1))\n",
    "\n",
    "threads = []\n",
    "n = 5\n",
    "for k in range (0, n):\n",
    "    t = threading.Thread(target=compute, args=(k+1,))\n",
    "    threads.append(t)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "\n",
    "for elem in language_model:\n",
    "    print(\"Frequenze per classe {} : {}\".format(elem[1].index(),elem[1]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus normalizzato per eliminare gli id degli autori che non hanno testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di normalizzazione per eliminare gli id degli autori che non hanno testi\n",
    "\n",
    "def normalize_authors(author_id):\n",
    "    index = classes.index(author_id)\n",
    "    return classes_norm[index]\n",
    "\n",
    "\n",
    "# Inizializzazione del dizionario dei corpus\n",
    "\n",
    "corpuses = {}\n",
    "for i in range (1, authors+1):\n",
    "    corpuses[i] = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # come author id prendo l'id normalizzato\n",
    "    i = normalize_authors(row[\"author\"])\n",
    "    corpuses[i].append(row[\"text\"])\n",
    "\n",
    "\n",
    "# stampa di prova, il dataset è troppo grande e provo a stampare solo qualcosa\n",
    "print(\"5 elementi del corpus classe 1    : {}\".format(corpuses[1][1:5]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET TRATTATO CREANDO DIVERSI MODELLI DI LINGUAGGIO PER OGNI AUTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Funzione per la lemmatizzazione di un testo in formato stringa\n",
    "\n",
    "def lemmatize_text(text, lemmatizer):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Creazione della distribuzione delle parole per i vari autori\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "\n",
    "\n",
    "language_model = []\n",
    "for i in range (0,authors):\n",
    "    language_model.append([])\n",
    "\n",
    "\n",
    "for i in range (1, authors+1):\n",
    "    \n",
    "    author_corpus = corpuses[i]\n",
    "    lemmatized_data = [lemmatize_text(text, lemmatizer) for text in author_corpus]\n",
    "    vectorized_data = vectorizer.fit_transform(lemmatized_data)\n",
    "\n",
    "    temp = []\n",
    "    temp.append(vectorizer.get_feature_names_out())\n",
    "    temp.append(vectorized_data)\n",
    "    language_model[i-1] = temp\n",
    "    \n",
    "\n",
    "    # print(vectorizer.get_feature_names_out())\n",
    "    # print(vectorized_data.toarray())\n",
    "    # print(\"Language model for author {} : {}\".format(z,vectorized_data.toarray().sum(axis=0)))\n",
    "    \n",
    "\n",
    "    print(\"{}/{}\".format(i,authors))\n",
    "    \n",
    "\n",
    "for index, elem in enumerate(language_model):\n",
    "    print(\"Modello di linguaggio per autore {} : {}\".format(index, elem[1].toarray().sum(axis=0)))\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
